{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup   # For parsing HTML/XML documents\n",
    "import requests                # For making HTTP requests to fetch web pages\n",
    "from urllib.parse import urlparse\n",
    "import random\n",
    "import time                   #For human like interaction\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch robots.txt. Status code: 403\n"
     ]
    }
   ],
   "source": [
    "robots_url = \"https://www.enterprise.com/robots.txt\"\n",
    "\n",
    "response = requests.get(robots_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.text)\n",
    "else:\n",
    "    print(\"Failed to fetch robots.txt. Status code:\", response.status_code)\n",
    "\n",
    "# Go to the site itselt to see its robot.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded robots.txt rules\n",
    "robots_txt = {\n",
    "    \"*\": [\n",
    "        \"/en/reserve.html\",\n",
    "        \"/es/reserve.html\",\n",
    "        \"/content/ecom/\",\n",
    "        \"/corporate-accounts\",\n",
    "        \"*/beta\",\n",
    "        \"/en/reserve/check-in.html\",\n",
    "        \"/es/reserve/check-in.html\",\n",
    "        \"/content/ecom/en/staging\",\n",
    "        \"/content/ecom/es/staging\",\n",
    "        \"/en/reserve/check-out.html\",\n",
    "        \"/es/reserve/check-out.html\",\n",
    "        \"/en/car-rental/campaigns/locations-near-me.html\",\n",
    "        \"/en/reserve/modify-rental.html\",\n",
    "        \"/en/reserve/univ-prep.html\",\n",
    "        \"/es/reserve/modify-rental.html\",\n",
    "        \"/en/account/account-activity.html\",\n",
    "        \"/en/account/request-missing-points.html\",\n",
    "        \"/es/account/account-activity.html\",\n",
    "        \"/es/account/request-missing-points.html\",\n",
    "        \"/en/reserve/view-modify-cancel/rental-search.html\",\n",
    "        \"/es/reserve/view-modify-cancel/rental-search.html\",\n",
    "        \"/en/communication-preferences.html\",\n",
    "        \"/en/communication-preferences/unsubscribe.html\",\n",
    "        \"/es/communication-preferences.html\",\n",
    "        \"/es/communication-preferences/unsubscribe.html\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def is_allowed(url):\n",
    "    # Parse the path from the URL\n",
    "    from urllib.parse import urlparse\n",
    "    path = urlparse(url).path\n",
    "\n",
    "    # Check if the path is disallowed for the given user-agent\n",
    "    for disallowed_url in robots_txt[\"*\"]:\n",
    "        if path.startswith(disallowed_url):\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def GET_UA():\n",
    "    uastrings = [\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1500.72 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10) AppleWebKit/600.1.25 (KHTML, like Gecko) Version/8.0 Safari/600.1.25\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:33.0) Gecko/20100101 Firefox/33.0\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/600.1.17 (KHTML, like Gecko) Version/7.1 Safari/537.85.10\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.3; WOW64; rv:33.0) Gecko/20100101 Firefox/33.0\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.104 Safari/537.36\"\n",
    "    ]\n",
    "    return random.choice(uastrings)\n",
    "\n",
    "def parse_url(url):\n",
    "    headers = {'User-Agent': GET_UA()}\n",
    "    response_code = None\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response_code = response.status_code\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", str(e))\n",
    "\n",
    "    return response_code\n",
    "\n",
    "# Modify the proxy_request function to use a proper endpoint and include authentication\n",
    "def proxy_request(url):\n",
    "    headers = {'User-Agent': GET_UA()}\n",
    "    proxy_ip = \"50.168.72.119\"\n",
    "    proxy_port = 31147\n",
    "    proxies = {\n",
    "    \"http\": f\"http://{proxy_ip}:{proxy_port}\",\n",
    "    \"https\": f\"http://{proxy_ip}:{proxy_port}\"\n",
    "    }\n",
    "    try:\n",
    "        # Send a request using the proxy\n",
    "        response = requests.get(url, headers=headers, proxies=proxies)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            return soup\n",
    "        else:\n",
    "            print(\"Request failed with status code:\", response.status_code)\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: HTTPSConnectionPool(host='www.enterprise.com', port=443): Max retries exceeded with url: /en/home.html (Caused by ProxyError('Unable to connect to proxy', ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001E4EF30E020>, 'Connection to 50.168.72.119 timed out. (connect timeout=None)')))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'status_code'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m response \u001b[38;5;241m=\u001b[39m proxy_request(url)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Check if the request was successful (status code 200)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_allowed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.enterprise.com/en/home.html\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Print the HTML content\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAND URL is allowed to be crawled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'status_code'"
     ]
    }
   ],
   "source": [
    "# URL to scrape\n",
    "url = \"https://www.enterprise.com/en/home.html\"\n",
    "\n",
    "# Send a GET request to fetch the HTML content\n",
    "response = proxy_request(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200 and is_allowed(\"https://www.enterprise.com/en/home.html\"):\n",
    "    # Print the HTML content\n",
    "    print(response.text, \"AND URL is allowed to be crawled.\")\n",
    "else:\n",
    "    print(\"Failed to fetch URL. Status code:\", response.status_code, \" OR URL is not allowed to be crawled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.enterprise.com\n",
      "Response Status Code: 200  AND URL is allowed to be crawled.\n",
      "https://www.enterprise.com/en/home.html\n",
      "Response Status Code: 200  AND URL is allowed to be crawled.\n",
      "https://www.enterprise.com/en/reserve.html#car_select\n",
      "Failed to fetch URL. Response status code: 200  OR URL is not allowed to be crawled.\n"
     ]
    }
   ],
   "source": [
    "url = [\"https://www.enterprise.com\", 'https://www.enterprise.com/en/home.html', 'https://www.enterprise.com/en/reserve.html#car_select']\n",
    "\n",
    "for urls in url:\n",
    "    response_code = parse_url(urls)\n",
    "    print(urls)\n",
    "    if response_code == 200 and is_allowed(urls):\n",
    "        print(\"Response Status Code:\", response_code, \" AND URL is allowed to be crawled.\")\n",
    "    else:   \n",
    "        print(\"Failed to fetch URL. Response status code:\", response_code, \" OR URL is not allowed to be crawled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 => FIRST PHASE -> COMPLETE\n",
      "\n",
      "<html><head>\n",
      "<title>Access Denied</title>\n",
      "</head><body>\n",
      "<h1>Access Denied</h1>\n",
      " \n",
      "You don't have permission to access \"http://www.enterprise.com/en/home.html\" on this server.<p>\n",
      "Reference #18.6c5d3a17.1711404007.4e9b84\n",
      "<p>https://errors.edgesuite.net/18.6c5d3a17.1711404007.4e9b84</p>\n",
      "</p></body>\n",
      "</html>\n",
      "\n",
      "\n",
      "Location input field not found.\n",
      "Location item not found.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Find the button and extract its URL or form action\u001b[39;00m\n\u001b[0;32m     37\u001b[0m button \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbutton\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcta cta--primary cta--extra-large cta--fullWidth cta--noMargin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m button_url \u001b[38;5;241m=\u001b[39m \u001b[43mbutton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# If the button is an anchor element\u001b[39;00m\n\u001b[0;32m     39\u001b[0m button_action \u001b[38;5;241m=\u001b[39m button\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformaction\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# If the button is a submit button within a form\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst.FIVE IF condition is successful\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "response_code = parse_url(url[1])\n",
    "print(response_code, \"=> FIRST PHASE -> COMPLETE\")\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response_code == 200:\n",
    "    # Send another request to fetch the full response\n",
    "    response = requests.get(url[1])\n",
    "    # Parse the HTML content\n",
    "    time.sleep(random.randint(2, 6)) \n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    print(\"\")\n",
    "    print(soup)\n",
    "    print(\"\")\n",
    "\n",
    "    # Find the location input field\n",
    "    location_input = soup.find(\"input\", class_=\"rs-input__field selected\")\n",
    "    time.sleep(random.randint(2, 6))  # Introduce a random delay\n",
    "\n",
    "    # Check if the location input field exists\n",
    "    if location_input:\n",
    "        # Set the value of the location input field (e.g., \"New York JFK International Airport\")\n",
    "        location_input[\"value\"] = \"New York JFK International Airport\"\n",
    "    else:\n",
    "        print(\"Location input field not found.\")\n",
    "\n",
    "    # Find the location item in the list and click on it\n",
    "    location_item = soup.find(\"li\", id=\"location-1018781\")\n",
    "    if location_item:\n",
    "        location_item.click()\n",
    "        time.sleep(2.6)  # Introduce a delay to simulate human-like interaction\n",
    "    else:\n",
    "        print(\"Location item not found.\")\n",
    "\n",
    "    # Find the button and extract its URL or form action\n",
    "    button = soup.find(\"button\", class_=\"cta cta--primary cta--extra-large cta--fullWidth cta--noMargin\")\n",
    "    button_url = button.get(\"href\")  # If the button is an anchor element\n",
    "    button_action = button.get(\"formaction\")  # If the button is a submit button within a form\n",
    "\n",
    "    print(\"First.FIVE IF condition is successful\")\n",
    "    # Send a POST request to submit the form data (if applicable)\n",
    "    if button_url:\n",
    "        response = requests.post(button_url, data={\"location\": \"New York JFK International Airport\"})\n",
    "        print(\"SECOND IF condition is successful\")\n",
    "    elif button_action:\n",
    "        response = requests.post(url, data={\"location\": \"New York JFK International Airport\"})\n",
    "        print(\"SECOND IF condition is successful\")\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the next page\n",
    "        next_page_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        print(\"SUCCESSFUL\")\n",
    "        # Extract relevant information from the next page (if needed)\n",
    "        # ...\n",
    "\n",
    "        # Print or process the extracted information\n",
    "        # ...\n",
    "else:\n",
    "    print(\"Failed to fetch URL. Status code:\", response_code)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time taken:\", end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: HTTPSConnectionPool(host='www.enterprise.com', port=443): Max retries exceeded with url: /en/home.html (Caused by ProxyError('Unable to connect to proxy', ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001E4EF23C5E0>, 'Connection to 50.168.72.119 timed out. (connect timeout=None)')))\n",
      "Failed to retrieve page content.\n"
     ]
    }
   ],
   "source": [
    "# To battle this, we are using PROXY\n",
    "# But as long it is a free proxy, there is less chance of scraping the website\n",
    "# Also idk why but my lap is hanging... IG because of 9+ desktop sites being opened my lap\n",
    "# IF ALLOWED SELENIUM, then I would have been able to scrape the website\n",
    "\n",
    "url = \"https://www.enterprise.com/en/home.html\"\n",
    "soup = proxy_request(url)\n",
    "if soup:\n",
    "    print(\"Page content retrieved successfully:\", soup)\n",
    "else:\n",
    "    print(\"Failed to retrieve page content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: HTTPSConnectionPool(host='www.enterprise.com', port=443): Max retries exceeded with url: /en/home.html (Caused by ProxyError('Unable to connect to proxy', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001E4EF30CDF0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it')))\n"
     ]
    }
   ],
   "source": [
    "# Define the proxy server details\n",
    "proxy_ip = \"64.227.4.90\"\n",
    "proxy_port = 31147\n",
    "\n",
    "# URL to access through the proxy\n",
    "url = \"https://www.enterprise.com/en/home.html\"\n",
    "\n",
    "# Set up the proxy configuration\n",
    "proxies = {\n",
    "    \"http\": f\"http://{proxy_ip}:{proxy_port}\",\n",
    "    \"https\": f\"http://{proxy_ip}:{proxy_port}\"\n",
    "}\n",
    "\n",
    "# Send a request using the proxy\n",
    "try:\n",
    "    response = requests.get(url, proxies=proxies)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        print(\"Request successful!\")\n",
    "\n",
    "        # Introduce additional delay\n",
    "        time.sleep(random.uniform(3, 6))\n",
    "\n",
    "        # Find the location input field\n",
    "        location_input = soup.find(\"input\", class_=\"rs-input__field selected\")\n",
    "\n",
    "        # Check if the location input field exists\n",
    "        if location_input:\n",
    "            # Set the value of the location input field\n",
    "            location_input[\"value\"] = \"New York JFK International Airport\"\n",
    "        else:\n",
    "            print(\"Location input field not found.\")\n",
    "\n",
    "        # Find the location item in the list and click on it\n",
    "        location_item = soup.find(\"li\", id=\"location-1018781\")\n",
    "        if location_item:\n",
    "            location_item.click()\n",
    "            # Introduce additional delay\n",
    "            time.sleep(2.6)\n",
    "        else:\n",
    "            print(\"Location item not found.\")\n",
    "\n",
    "    else:\n",
    "        print(\"Request failed with status code:\", response.status_code)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
