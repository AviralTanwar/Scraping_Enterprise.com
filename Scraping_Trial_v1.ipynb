{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup   # For parsing HTML/XML documents\n",
    "import requests                # For making HTTP requests to fetch web pages\n",
    "from urllib.parse import urlparse\n",
    "import random\n",
    "import time                   #For human like interaction\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch robots.txt. Status code: 403\n"
     ]
    }
   ],
   "source": [
    "robots_url = \"https://www.enterprise.com/robots.txt\"\n",
    "\n",
    "response = requests.get(robots_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.text)\n",
    "else:\n",
    "    print(\"Failed to fetch robots.txt. Status code:\", response.status_code)\n",
    "\n",
    "# Go to the site itselt to see its robot.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded robots.txt rules\n",
    "robots_txt = {\n",
    "    \"*\": [\n",
    "        \"/en/reserve.html\",\n",
    "        \"/es/reserve.html\",\n",
    "        \"/content/ecom/\",\n",
    "        \"/corporate-accounts\",\n",
    "        \"*/beta\",\n",
    "        \"/en/reserve/check-in.html\",\n",
    "        \"/es/reserve/check-in.html\",\n",
    "        \"/content/ecom/en/staging\",\n",
    "        \"/content/ecom/es/staging\",\n",
    "        \"/en/reserve/check-out.html\",\n",
    "        \"/es/reserve/check-out.html\",\n",
    "        \"/en/car-rental/campaigns/locations-near-me.html\",\n",
    "        \"/en/reserve/modify-rental.html\",\n",
    "        \"/en/reserve/univ-prep.html\",\n",
    "        \"/es/reserve/modify-rental.html\",\n",
    "        \"/en/account/account-activity.html\",\n",
    "        \"/en/account/request-missing-points.html\",\n",
    "        \"/es/account/account-activity.html\",\n",
    "        \"/es/account/request-missing-points.html\",\n",
    "        \"/en/reserve/view-modify-cancel/rental-search.html\",\n",
    "        \"/es/reserve/view-modify-cancel/rental-search.html\",\n",
    "        \"/en/communication-preferences.html\",\n",
    "        \"/en/communication-preferences/unsubscribe.html\",\n",
    "        \"/es/communication-preferences.html\",\n",
    "        \"/es/communication-preferences/unsubscribe.html\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def is_allowed(url):\n",
    "    # Parse the path from the URL\n",
    "    path = urlparse(url).path\n",
    "\n",
    "    # Check if the path is disallowed for the given user-agent\n",
    "    for disallowed_url in robots_txt[\"*\"]:\n",
    "        if path.startswith(disallowed_url):\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def GET_UA():\n",
    "    uastrings = [\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36\",\\\n",
    "                \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1500.72 Safari/537.36\",\\\n",
    "                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10) AppleWebKit/600.1.25 (KHTML, like Gecko) Version/8.0 Safari/600.1.25\",\\\n",
    "                \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:33.0) Gecko/20100101 Firefox/33.0\",\\\n",
    "                \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36\",\\\n",
    "                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36\",\\\n",
    "                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/600.1.17 (KHTML, like Gecko) Version/7.1 Safari/537.85.10\",\\\n",
    "                \"Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko\",\\\n",
    "                \"Mozilla/5.0 (Windows NT 6.3; WOW64; rv:33.0) Gecko/20100101 Firefox/33.0\",\\\n",
    "                \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.104 Safari/537.36\"\\\n",
    "                ]\n",
    "    return random.choice(uastrings)\n",
    "\n",
    "def parse_url(url):\n",
    "    headers = {'User-Agent': GET_UA()}\n",
    "    response_code = None\n",
    " \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response_code = response.status_code\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", str(e))\n",
    " \n",
    "    return response_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch URL. Status code: 403  OR URL is not allowed to be crawled.\n"
     ]
    }
   ],
   "source": [
    "# URL to scrape\n",
    "url = \"https://www.enterprise.com/en/home.html\"\n",
    "\n",
    "# Send a GET request to fetch the HTML content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200 and is_allowed(\"https://www.enterprise.com/en/home.html\"):\n",
    "    # Print the HTML content\n",
    "    print(response.text, \"AND URL is allowed to be crawled.\")\n",
    "else:\n",
    "    print(\"Failed to fetch URL. Status code:\", response.status_code, \" OR URL is not allowed to be crawled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.enterprise.com\n",
      "Response Status Code: 200  AND URL is allowed to be crawled.\n",
      "https://www.enterprise.com/en/home.html\n",
      "Response Status Code: 200  AND URL is allowed to be crawled.\n",
      "https://www.enterprise.com/en/reserve.html#car_select\n",
      "Failed to fetch URL. Response status code: 200  OR URL is not allowed to be crawled.\n"
     ]
    }
   ],
   "source": [
    "url = [\"https://www.enterprise.com\", 'https://www.enterprise.com/en/home.html', 'https://www.enterprise.com/en/reserve.html#car_select']\n",
    "\n",
    "for urls in url:\n",
    "    response_code = parse_url(urls)\n",
    "    print(urls)\n",
    "    if response_code == 200 and is_allowed(urls):\n",
    "        print(\"Response Status Code:\", response_code, \" AND URL is allowed to be crawled.\")\n",
    "    else:   \n",
    "        print(\"Failed to fetch URL. Response status code:\", response_code, \" OR URL is not allowed to be crawled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESSFUL 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSUCCESSFUL 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Parse the HTML content\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Find the location input field\u001b[39;00m\n\u001b[0;32m     10\u001b[0m location_input \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrs-input__field selected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "response = parse_url(url[1])\n",
    "print(\"SUCCESSFUL 1\")\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "print(\"SUCCESSFUL 2\")\n",
    "\n",
    "# Find the location input field\n",
    "location_input = soup.find(\"input\", class_=\"rs-input__field selected\")\n",
    "sleep(random.randint(2, 6))\n",
    "\n",
    "# Check if the location input field exists\n",
    "if location_input:\n",
    "    # Set the value of the location input field (e.g., \"New York JFK International Airport\")\n",
    "    location_input[\"value\"] = \"New York JFK International Airport\"\n",
    "else:\n",
    "    print(\"Location input field not found.\")\n",
    "\n",
    "# Find the location item in the list and click on it\n",
    "location_item = soup.find(\"li\", id=\"location-1018781\")\n",
    "if location_item:\n",
    "    location_item.click()\n",
    "    time.sleep(2.6)  # Introduce a delay to simulate human-like interaction\n",
    "else:\n",
    "    print(\"Location item not found.\")\n",
    "\n",
    "# Find the button and extract its URL or form action\n",
    "button = soup.find(\"button\", class_=\"cta cta--primary cta--extra-large cta--fullWidth cta--noMargin\")\n",
    "button_url = button.get(\"href\")  # If the button is an anchor element\n",
    "button_action = button.get(\"formaction\")  # If the button is a submit button within a form\n",
    "    \n",
    "print(\"First.FIVE IF condition is successful\")\n",
    "# Send a POST request to submit the form data (if applicable)\n",
    "if button_url:\n",
    "    response = requests.post(button_url, data={\"location\": \"New York JFK International Airport\"})\n",
    "    print(\"SECOND IF condition is successful\")\n",
    "elif button_action:\n",
    "    response = requests.post(url, data={\"location\": \"New York JFK International Airport\"})\n",
    "    print(\"SECOND IF condition is successful\")\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the next page\n",
    "    next_page_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    print(\"SUCCESSFUL\")\n",
    "    # Extract relevant information from the next page (if needed)\n",
    "    # ...\n",
    "\n",
    "    # Print or process the extracted information\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESSFUL 1\n",
      "SUCCESSFUL 2\n",
      "Location input field not found.\n",
      "Location item not found.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Find the button and extract its URL or form action\u001b[39;00m\n\u001b[0;32m     37\u001b[0m button \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbutton\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcta cta--primary cta--extra-large cta--fullWidth cta--noMargin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m button_url \u001b[38;5;241m=\u001b[39m \u001b[43mbutton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# If the button is an anchor element\u001b[39;00m\n\u001b[0;32m     39\u001b[0m button_action \u001b[38;5;241m=\u001b[39m button\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformaction\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# If the button is a submit button within a form\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst.FIVE IF condition is successful\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "response_code = parse_url(url[1])\n",
    "print(\"SUCCESSFUL 1\")\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response_code == 200:\n",
    "    # Send another request to fetch the full response\n",
    "    response = requests.get(url[1])\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    print(\"SUCCESSFUL 2\")\n",
    "\n",
    "    # Find the location input field\n",
    "    location_input = soup.find(\"input\", class_=\"rs-input__field selected\")\n",
    "    time.sleep(random.randint(2, 6))  # Introduce a random delay\n",
    "\n",
    "    # Check if the location input field exists\n",
    "    if location_input:\n",
    "        # Set the value of the location input field (e.g., \"New York JFK International Airport\")\n",
    "        location_input[\"value\"] = \"New York JFK International Airport\"\n",
    "    else:\n",
    "        print(\"Location input field not found.\")\n",
    "\n",
    "    # Find the location item in the list and click on it\n",
    "    location_item = soup.find(\"li\", id=\"location-1018781\")\n",
    "    if location_item:\n",
    "        location_item.click()\n",
    "        time.sleep(2.6)  # Introduce a delay to simulate human-like interaction\n",
    "    else:\n",
    "        print(\"Location item not found.\")\n",
    "\n",
    "    # Find the button and extract its URL or form action\n",
    "    button = soup.find(\"button\", class_=\"cta cta--primary cta--extra-large cta--fullWidth cta--noMargin\")\n",
    "    button_url = button.get(\"href\")  # If the button is an anchor element\n",
    "    button_action = button.get(\"formaction\")  # If the button is a submit button within a form\n",
    "\n",
    "    print(\"First.FIVE IF condition is successful\")\n",
    "    # Send a POST request to submit the form data (if applicable)\n",
    "    if button_url:\n",
    "        response = requests.post(button_url, data={\"location\": \"New York JFK International Airport\"})\n",
    "        print(\"SECOND IF condition is successful\")\n",
    "    elif button_action:\n",
    "        response = requests.post(url, data={\"location\": \"New York JFK International Airport\"})\n",
    "        print(\"SECOND IF condition is successful\")\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the next page\n",
    "        next_page_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        print(\"SUCCESSFUL\")\n",
    "        # Extract relevant information from the next page (if needed)\n",
    "        # ...\n",
    "\n",
    "        # Print or process the extracted information\n",
    "        # ...\n",
    "else:\n",
    "    print(\"Failed to fetch URL. Status code:\", response_code)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time taken:\", end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><head>\n",
      "<title>Access Denied</title>\n",
      "</head><body>\n",
      "<h1>Access Denied</h1>\n",
      " \n",
      "You don't have permission to access \"http://www.enterprise.com/en/home.html\" on this server.<p>\n",
      "Reference #18.17a81160.1711300006.2e334a8d\n",
      "</p></body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
